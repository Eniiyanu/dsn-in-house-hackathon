{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel Approach for Low-Resource NMT: Efficient Fine-Tuning with Data Augmentation and Quality Control\n",
    "\n",
    "Based on deep research into low-resource NMT for African languages (Yoruba, Igbo, Hausa to English), the following novel approach combines:\n",
    "- Multilingual fine-tuning of a distilled NLLB-200 model (optimized for African languages) using LoRA for parameter efficiency.\n",
    "- Data quality control: Filter noisy samples based on length ratios, repeated characters, unicode normalization, and deduplication.\n",
    "- Data augmentation via back-translation and external datasets (JW300, FLORES-101, Masakhane).\n",
    "- Curriculum learning: Train first on high-confidence pairs.\n",
    "- Evaluation: Use WER alongside BLEU for robustness.\n",
    "- Robustness: Added error handling, validation, and optimized for T4 (fp16, batch sizes).\n",
    "- Submission conforms to format: ID,Translation with quoted strings.\n",
    "- This leverages recent insights from papers on GANs/augmentation, federated fine-tuning, and domain adaptation.\n",
    "\n",
    "Requirements: Install transformers, datasets, evaluate, jiwer, peft, torch, opustools-pkg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T15:52:40.985846Z",
     "iopub.status.busy": "2025-10-23T15:52:40.985580Z",
     "iopub.status.idle": "2025-10-23T15:52:46.850788Z",
     "shell.execute_reply": "2025-10-23T15:52:46.849851Z",
     "shell.execute_reply.started": "2025-10-23T15:52:40.985826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers\" \"datasets\" \"evaluate\" \"jiwer\" \"peft\" \"torch\" \"opustools-pkg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T15:52:52.183018Z",
     "iopub.status.busy": "2025-10-23T15:52:52.182237Z",
     "iopub.status.idle": "2025-10-23T15:53:05.618592Z",
     "shell.execute_reply": "2025-10-23T15:53:05.617868Z",
     "shell.execute_reply.started": "2025-10-23T15:52:52.182986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq, EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from evaluate import load\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"/kaggle/input/dsn-in-house/\")\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    train_df = pd.read_csv(data_dir / train_file)\n",
    "    test_df = pd.read_csv(data_dir / test_file)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"Data files not found: {e}\")\n",
    "\n",
    "# Preprocessing functions\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFKC', text.strip())\n",
    "\n",
    "def is_noisy(text, lang):\n",
    "    if not isinstance(text, str) or len(text) < 5 or len(text) > 500:\n",
    "        return True\n",
    "    if re.search(r'(.)\\1{10,}', text):  # Fixed: \\1 for backreference (repeated chars)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def compute_length_ratio(src, tgt):\n",
    "    src_len = len(src.split())\n",
    "    tgt_len = len(tgt.split())\n",
    "    return tgt_len / max(src_len, 1)\n",
    "\n",
    "# Clean and filter train data\n",
    "train_df['input'] = train_df['input'].astype(str).apply(normalize_text)\n",
    "train_df['Output'] = train_df['Output'].astype(str).apply(normalize_text)\n",
    "train_df = train_df.dropna()\n",
    "train_df = train_df.drop_duplicates(subset=['input', 'Output'])\n",
    "train_df = train_df[~train_df.apply(lambda row: is_noisy(row['input'], row['Language']) or is_noisy(row['Output'], row['Language']), axis=1)]\n",
    "train_df['length_ratio'] = train_df.apply(lambda row: compute_length_ratio(row['input'], row['Output']), axis=1)\n",
    "train_df = train_df[(train_df['length_ratio'] >= 0.5) & (train_df['length_ratio'] <= 3.0)]\n",
    "\n",
    "# Split train/val\n",
    "train_split = []\n",
    "val_split = []\n",
    "for lang in train_df['Language'].unique():\n",
    "    lang_df = train_df[train_df['Language'] == lang]\n",
    "    split_idx = int(0.8 * len(lang_df))\n",
    "    train_split.append(lang_df[:split_idx])\n",
    "    val_split.append(lang_df[split_idx:])\n",
    "train_df = pd.concat(train_split).reset_index(drop=True)\n",
    "val_df = pd.concat(val_split).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load external datasets for augmentation\n",
    "# JW300 via OPUS\n",
    "!pip install opustools-pkg\n",
    "import os\n",
    "os.system(\"opus_read -d JW300 -s en -t yo -wm moses -w jw300.yo.en\")\n",
    "jw_yo = pd.read_csv('jw300.yo.en', sep='\\t', header=None, names=['input', 'Output'])\n",
    "jw_yo['Language'] = 'Yoruba'\n",
    "os.system(\"opus_read -d JW300 -s en -t ig -wm moses -w jw300.ig.en\")\n",
    "jw_ig = pd.read_csv('jw300.ig.en', sep='\\t', header=None, names=['input', 'Output'])\n",
    "jw_ig['Language'] = 'Igbo'\n",
    "os.system(\"opus_read -d JW300 -s en -t ha -wm moses -w jw300.ha.en\")\n",
    "jw_ha = pd.read_csv('jw300.ha.en', sep='\\t', header=None, names=['input', 'Output'])\n",
    "jw_ha['Language'] = 'Hausa'\n",
    "jw_df = pd.concat([jw_yo, jw_ig, jw_ha]).reset_index(drop=True)\n",
    "\n",
    "# FLORES-101 (or download manually and load locally since HF loading fails)\n",
    "!wget -O flores101_dataset.tar.gz https://dl.fbaipublicfiles.com/flores101/dataset/flores101_dataset.tar.gz\n",
    "!tar -xzf flores101_dataset.tar.gz\n",
    "\n",
    "# Load devtest files for Yoruba, Igbo, Hausa\n",
    "import glob\n",
    "flores_files = {\n",
    "    'Yoruba': 'flores101_dataset/devtest/yor.devtest',\n",
    "    'Igbo': 'flores101_dataset/devtest/ibo.devtest',\n",
    "    'Hausa': 'flores101_dataset/devtest/hau.devtest'\n",
    "}\n",
    "\n",
    "flores_data = []\n",
    "for lang, file_path in flores_files.items():\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # Assuming each line is a sentence, pair with English (need eng.devtest)\n",
    "    eng_file = file_path.replace(lang.lower()[:3], 'eng')\n",
    "    if os.path.exists(eng_file):\n",
    "        with open(eng_file, 'r', encoding='utf-8') as ef:\n",
    "            eng_lines = ef.readlines()\n",
    "        for src, tgt in zip(lines, eng_lines):\n",
    "            flores_data.append({'input': src.strip(), 'Output': tgt.strip(), 'Language': lang})\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "flores_df = pd.DataFrame(flores_data)\n",
    "\n",
    "# Augment\n",
    "aug_df = pd.concat([jw_df.sample(frac=0.05), flores_df]).reset_index(drop=True)  # 5% JW, all FLORES\n",
    "aug_df['input'] = aug_df['input'].apply(normalize_text)\n",
    "aug_df['Output'] = aug_df['Output'].apply(normalize_text)\n",
    "train_df = pd.concat([train_df, aug_df]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert to Datasets\n",
    "train_dataset = Dataset.from_pandas(train_df[['input', 'Output', 'Language', 'length_ratio']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['input', 'Output', 'Language', 'length_ratio']])\n",
    "test_df = test_df.rename(columns={'Input Text': 'input', 'Competition_ID': 'ID'})\n",
    "test_df = test_df.dropna()\n",
    "test_dataset = Dataset.from_pandas(test_df[['ID', 'input', 'Language']])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Model and Tokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Failed to load model/tokenizer: {e}\")\n",
    "\n",
    "lang_to_code = {'Yoruba': 'yor_Latn', 'Igbo': 'ibo_Latn', 'Hausa': 'hau_Latn'}\n",
    "tgt_lang = 'eng_Latn'\n",
    "\n",
    "# Set tgt_lang globally\n",
    "tokenizer.tgt_lang = tgt_lang\n",
    "\n",
    "# Sort datasets by Language to group batches\n",
    "dataset['train'] = dataset['train'].sort('Language')\n",
    "dataset['validation'] = dataset['validation'].sort('Language')\n",
    "dataset['test'] = dataset['test'].sort('Language')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    if 'Output' in examples:\n",
    "        targets = examples['Output']\n",
    "    else:\n",
    "        targets = None\n",
    "\n",
    "    # Set src_lang per batch (now grouped by Language)\n",
    "    if examples['Language']:\n",
    "        tokenizer.src_lang = lang_to_code[examples['Language'][0]]\n",
    "\n",
    "    if targets is not None:\n",
    "        model_inputs = tokenizer(\n",
    "            inputs, text_target=targets, max_length=128, truncation=True\n",
    "        )\n",
    "        # Compute length_ratio for curriculum\n",
    "        src_lens = [len(text.split()) for text in inputs]\n",
    "        tgt_lens = [len(text.split()) for text in targets]\n",
    "        ratios = [tgt / max(src, 1) for src, tgt in zip(src_lens, tgt_lens)]\n",
    "        model_inputs['length_ratio'] = ratios\n",
    "    else:\n",
    "        model_inputs = tokenizer(\n",
    "            inputs, max_length=128, truncation=True\n",
    "        )\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Starting dataset mapping...\")\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "print(\"Mapping completed.\")\n",
    "\n",
    "# Remove columns after mapping, conditionally\n",
    "columns_to_remove_train_val = ['input', 'Language', 'Output']\n",
    "columns_to_remove_test = ['input', 'Language']\n",
    "\n",
    "tokenized_datasets['train'] = tokenized_datasets['train'].remove_columns([col for col in columns_to_remove_train_val if col in tokenized_datasets['train'].column_names])\n",
    "tokenized_datasets['validation'] = tokenized_datasets['validation'].remove_columns([col for col in columns_to_remove_train_val if col in tokenized_datasets['validation'].column_names])\n",
    "tokenized_datasets['test'] = tokenized_datasets['test'].remove_columns([col for col in columns_to_remove_test if col in tokenized_datasets['test'].column_names])\n",
    "\n",
    "if 'length_ratio' in tokenized_datasets['train'].column_names:\n",
    "    tokenized_datasets['train'] = tokenized_datasets['train'].sort('length_ratio').remove_columns('length_ratio')\n",
    "if 'length_ratio' in tokenized_datasets['validation'].column_names:\n",
    "    tokenized_datasets['validation'] = tokenized_datasets['validation'].remove_columns('length_ratio')\n",
    "\n",
    "print(\"Applying LoRA...\")\n",
    "# LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA applied.\")\n",
    "\n",
    "# Back-translation (optimized: reduce frac to 0.05 for speed)\n",
    "try:\n",
    "    back_trans_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "except:\n",
    "    back_trans_model = model\n",
    "\n",
    "def back_translate(english_texts, src_lang):\n",
    "    inputs = tokenizer(english_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    translated = back_trans_model.generate(\n",
    "        **inputs, \n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(src_lang),\n",
    "        max_length=128\n",
    "    )\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "print(\"Starting back-translation...\")\n",
    "augmented_data = []\n",
    "for lang in ['Yoruba', 'Igbo', 'Hausa']:\n",
    "    print(f\"Processing {lang}...\")\n",
    "    lang_code = lang_to_code[lang]\n",
    "    eng_samples = val_df[val_df['Language'] == lang]['Output'].sample(frac=0.03, random_state=42).tolist()  # Reduced to 5%\n",
    "    if eng_samples:\n",
    "        pseudo_src = back_translate(eng_samples, lang_code)\n",
    "        for src, tgt in zip(pseudo_src, eng_samples):\n",
    "            augmented_data.append({'input': src, 'Output': tgt, 'Language': lang, 'length_ratio': compute_length_ratio(src, tgt)})\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "if augmented_data:\n",
    "    aug_df_bt = pd.DataFrame(augmented_data)\n",
    "    aug_dataset_bt = Dataset.from_pandas(aug_df_bt)\n",
    "    aug_tokenized_bt = aug_dataset_bt.map(preprocess_function, batched=True, remove_columns=['input', 'Output', 'Language', 'length_ratio'])\n",
    "    \n",
    "    # Use concatenate_datasets instead of .concatenate()\n",
    "    tokenized_datasets['train'] = concatenate_datasets([tokenized_datasets['train'], aug_tokenized_bt])\n",
    "\n",
    "print(\"Back-translation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load\n",
    "\n",
    "# Clear memory\n",
    "torch.cuda.empty_cache()\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Load and prepare data\n",
    "train_dataset = Dataset.from_pandas(train_df[['input', 'Output', 'Language']])\n",
    "val_dataset = Dataset.from_pandas(val_df[['input', 'Output', 'Language']])\n",
    "test_df = test_df.rename(columns={'Input Text': 'input', 'Competition_ID': 'ID'})\n",
    "test_df = test_df.dropna()\n",
    "test_dataset = Dataset.from_pandas(test_df[['ID', 'input', 'Language']])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Model and Tokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "lang_to_code = {'Yoruba': 'yor_Latn', 'Igbo': 'ibo_Latn', 'Hausa': 'hau_Latn'}\n",
    "tgt_lang = 'eng_Latn'\n",
    "tokenizer.tgt_lang = tgt_lang\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    targets = examples.get('Output', None)\n",
    "    \n",
    "    if examples['Language']:\n",
    "        tokenizer.src_lang = lang_to_code[examples['Language'][0]]\n",
    "    \n",
    "    if targets is not None:\n",
    "        model_inputs = tokenizer(\n",
    "            inputs, \n",
    "            text_target=targets, \n",
    "            max_length=64,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "    else:\n",
    "        model_inputs = tokenizer(\n",
    "            inputs, \n",
    "            max_length=64, \n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "    return model_inputs\n",
    "\n",
    "# SKIP BACK-TRANSLATION\n",
    "print(\"Skipping back-translation...\")\n",
    "\n",
    "# Tokenize datasets - FIXED\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "tokenized_train = dataset['train'].map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=['input', 'Output', 'Language']\n",
    ")\n",
    "\n",
    "tokenized_val = dataset['validation'].map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=['input', 'Output', 'Language']\n",
    ")\n",
    "\n",
    "tokenized_test = dataset['test'].map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=['input', 'Language']  # Keep 'ID' for test\n",
    ")\n",
    "\n",
    "tokenized_datasets = DatasetDict({\n",
    "    'train': tokenized_train,\n",
    "    'validation': tokenized_val,\n",
    "    'test': tokenized_test\n",
    "})\n",
    "\n",
    "print(\"Tokenization completed.\")\n",
    "\n",
    "# LoRA\n",
    "print(\"Applying LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"LoRA applied.\")\n",
    "\n",
    "# Fast Training Arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=200,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    dataloader_num_workers=2,\n",
    "    logging_steps=50,\n",
    "    report_to=[\"none\"],\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(f\"Validation Loss: {results['eval_loss']:.4f}, WER: {results['eval_wer']:.4f}\")\n",
    "\n",
    "trainer.save_model(\"./final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers==4.44.2 huggingface-hub==0.24.6 peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T16:11:50.779631Z",
     "iopub.status.busy": "2025-10-23T16:11:50.779322Z",
     "iopub.status.idle": "2025-10-23T16:12:36.930960Z",
     "shell.execute_reply": "2025-10-23T16:12:36.930239Z",
     "shell.execute_reply.started": "2025-10-23T16:11:50.779607Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "from datasets import Dataset, DatasetDict\n",
    "import re\n",
    "import transformers.utils.hub as hub\n",
    "\n",
    "# ---- Fix Hugging Face 404 ----\n",
    "def safe_list_repo_templates(*args, **kwargs):\n",
    "    return []\n",
    "hub.list_repo_templates = safe_list_repo_templates\n",
    "\n",
    "# ---- Paths ----\n",
    "data_dir = Path(\"/kaggle/input/dsn-in-house/\")\n",
    "test_file = data_dir / \"test.csv\"\n",
    "if not test_file.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {test_file}\")\n",
    "\n",
    "# ---- Load model ----\n",
    "base_model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "adapter_path = \"final_model\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)\n",
    "print(\"Tokenizer loaded\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "if os.path.exists(adapter_path):\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    print(\"LoRA adapter loaded\")\n",
    "else:\n",
    "    print(\"No LoRA adapter found; using base model\")\n",
    "    model = base_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device).eval()\n",
    "if device == \"cuda\":\n",
    "    model.half()\n",
    "print(f\"Model ready on {device}\")\n",
    "\n",
    "# ---- Translation ----\n",
    "lang_to_code = {'Yoruba': 'yor_Latn', 'Igbo': 'ibo_Latn', 'Hausa': 'hau_Latn'}\n",
    "tgt_lang = 'eng_Latn'\n",
    "\n",
    "def generate_translation(batch):\n",
    "    inputs = batch['input']\n",
    "    languages = batch['Language']\n",
    "    src_lang = lang_to_code[languages[0]]\n",
    "    tokenizer.src_lang = src_lang\n",
    "\n",
    "    encoded = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **encoded,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "            max_length=128,\n",
    "            num_beams=4,\n",
    "            temperature=0.5,\n",
    "            length_penalty=1.0,\n",
    "            repetition_penalty=1.1,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    return {'translation': tokenizer.batch_decode(outputs, skip_special_tokens=True)}\n",
    "\n",
    "# ---- Load test data ----\n",
    "test_df = pd.read_csv(test_file)\n",
    "test_df = test_df.rename(columns={'Input Text': 'input', 'Competition_ID': 'ID'}).dropna()\n",
    "dataset = DatasetDict({'test': Dataset.from_pandas(test_df[['ID', 'input', 'Language']])})\n",
    "\n",
    "\n",
    "test_with_translations = dataset[\"test\"].map(\n",
    "    generate_translation,\n",
    "    batched=True,\n",
    "    batch_size=16,\n",
    "    remove_columns=['input', 'Language']\n",
    ")\n",
    "\n",
    "# ---- Postprocess ----\n",
    "def clean_text(t):\n",
    "    t = re.sub(r'\\s+([?.!,])', r'\\1', t)\n",
    "    return re.sub(r'\\s{2,}', ' ', t).strip()\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_with_translations['ID'],\n",
    "    'Output text': [clean_text(x) for x in test_with_translations['translation']]\n",
    "})\n",
    "\n",
    "output_path = \"/kaggle/working/submission.csv\"\n",
    "submission_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"\\nDone! Submission saved:\", output_path)\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8548067,
     "sourceId": 13466127,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
